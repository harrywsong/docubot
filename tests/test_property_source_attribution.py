"""
Property-based test for source attribution.

Feature: rag-chatbot-with-vision, Property 23: Response Source Attribution

Validates: Requirements 8.6, 8.7

**Validates: Requirements 8.6, 8.7**

This test verifies that for any response generated by the query engine,
the response includes references to the source documents and chunks used
to generate the answer.
"""

import pytest
from hypothesis import given, strategies as st, settings, assume
from unittest.mock import patch, Mock
from backend.query_engine import QueryEngine
from backend.models import QueryResult


# Custom strategies for generating test data
@st.composite
def query_result_strategy(draw):
    """Generate a QueryResult with realistic data."""
    chunk_id = draw(st.text(min_size=1, max_size=50, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'P'))))
    content = draw(st.text(min_size=10, max_size=1000))
    filename = draw(st.text(min_size=1, max_size=100, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd'))))
    
    # Add file extension
    file_ext = draw(st.sampled_from(['.pdf', '.txt', '.jpg', '.png', '.jpeg']))
    filename = filename + file_ext
    
    file_type = 'image' if file_ext in ['.jpg', '.png', '.jpeg'] else 'text'
    folder_path = draw(st.text(min_size=1, max_size=200, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'P'))))
    # Generate similarity scores >= 0.5 to ensure they pass the filtering threshold
    similarity_score = draw(st.floats(min_value=0.5, max_value=1.0))
    
    metadata = {
        'filename': filename,
        'file_type': file_type,
        'folder_path': folder_path
    }
    
    # Add type-specific metadata
    if file_type == 'image':
        # Add image-specific metadata (merchant, date, amount)
        if draw(st.booleans()):
            metadata['merchant'] = draw(st.text(min_size=1, max_size=50, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd'))))
        if draw(st.booleans()):
            year = draw(st.integers(min_value=2020, max_value=2030))
            month = draw(st.integers(min_value=1, max_value=12))
            day = draw(st.integers(min_value=1, max_value=28))
            metadata['date'] = f"{year:04d}-{month:02d}-{day:02d}"
        if draw(st.booleans()):
            metadata['total_amount'] = draw(st.floats(min_value=0.01, max_value=10000.0))
    else:
        # Add text-specific metadata (page number)
        if draw(st.booleans()):
            metadata['page_number'] = draw(st.integers(min_value=1, max_value=1000))
    
    return QueryResult(
        chunk_id=chunk_id,
        content=content,
        metadata=metadata,
        similarity_score=similarity_score
    )


@st.composite
def query_results_list_strategy(draw):
    """Generate a list of QueryResults (1-10 results)."""
    num_results = draw(st.integers(min_value=1, max_value=10))
    return [draw(query_result_strategy()) for _ in range(num_results)]


@st.composite
def question_strategy(draw):
    """Generate a realistic question string."""
    # Generate questions that are not spending queries to avoid aggregation logic
    question_templates = [
        "What information do you have about {}?",
        "Tell me about {}",
        "Show me documents related to {}",
        "What does the document say about {}?",
        "Find information on {}",
        "Search for {}"
    ]
    
    template = draw(st.sampled_from(question_templates))
    topic = draw(st.text(min_size=1, max_size=50, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd'))))
    
    return template.format(topic)


class TestPropertySourceAttribution:
    """
    Property-based tests for source attribution.
    
    Feature: rag-chatbot-with-vision, Property 23: Response Source Attribution
    """
    
    @given(
        question=question_strategy(),
        results=query_results_list_strategy()
    )
    @settings(max_examples=100, deadline=None)
    def test_response_includes_source_references(self, question, results):
        """
        Property 23: Response Source Attribution
        
        For any response generated by the query engine, the response should include
        references to the source documents and chunks used to generate the answer.
        
        This test verifies that:
        1. The response includes a 'sources' field
        2. The sources list is not empty when results are available
        3. Each source includes filename, chunk content, and similarity score
        4. The number of sources matches the number of results
        """
        # Arrange: Create query engine with mocked dependencies
        with patch('backend.query_engine.get_embedding_engine') as mock_emb:
            with patch('backend.query_engine.get_vector_store') as mock_vs:
                # Mock embedding generation
                mock_emb.return_value.generate_embedding.return_value = [0.1] * 384
                
                # Mock vector store to return our generated results
                mock_vs.return_value.query.return_value = results
                
                engine = QueryEngine()
                
                # Act: Execute query
                response = engine.query(question)
                
                # Assert: Verify source attribution properties
                
                # Property 1: Response must include 'sources' field
                assert 'sources' in response, "Response must include 'sources' field"
                
                # Property 2: Sources list must not be empty when results are available
                assert len(response['sources']) > 0, "Sources list must not be empty when results are available"
                
                # Property 3: Number of sources should match number of results
                assert len(response['sources']) == len(results), \
                    f"Number of sources ({len(response['sources'])}) must match number of results ({len(results)})"
                
                # Property 4: Each source must include required fields
                for i, source in enumerate(response['sources']):
                    # Check required fields exist
                    assert 'filename' in source, f"Source {i} must include 'filename'"
                    assert 'chunk' in source, f"Source {i} must include 'chunk'"
                    assert 'score' in source, f"Source {i} must include 'score'"
                    assert 'metadata' in source, f"Source {i} must include 'metadata'"
                    
                    # Check field types
                    assert isinstance(source['filename'], str), f"Source {i} filename must be a string"
                    assert isinstance(source['chunk'], str), f"Source {i} chunk must be a string"
                    assert isinstance(source['score'], (int, float)), f"Source {i} score must be numeric"
                    assert isinstance(source['metadata'], dict), f"Source {i} metadata must be a dictionary"
                    
                    # Check that filename matches the original result
                    assert source['filename'] == results[i].metadata.get('filename', 'Unknown'), \
                        f"Source {i} filename must match result metadata"
                    
                    # Check that chunk content is from the original result
                    # (may be truncated, so check if it's a prefix)
                    original_content = results[i].content
                    chunk_content = source['chunk'].replace("...", "")  # Remove truncation marker
                    assert chunk_content in original_content or original_content in chunk_content, \
                        f"Source {i} chunk must contain content from original result"
                    
                    # Check that score matches (rounded to 3 decimal places)
                    expected_score = round(results[i].similarity_score, 3)
                    assert source['score'] == expected_score, \
                        f"Source {i} score must match result similarity score (expected {expected_score}, got {source['score']})"
    
    @given(question=question_strategy())
    @settings(max_examples=100, deadline=None)
    def test_response_with_no_results_has_empty_sources(self, question):
        """
        Test that when no results are found, sources list is empty.
        
        This is a boundary case for Property 23.
        """
        # Arrange: Create query engine with mocked dependencies
        with patch('backend.query_engine.get_embedding_engine') as mock_emb:
            with patch('backend.query_engine.get_vector_store') as mock_vs:
                # Mock embedding generation
                mock_emb.return_value.generate_embedding.return_value = [0.1] * 384
                
                # Mock vector store to return no results
                mock_vs.return_value.query.return_value = []
                
                engine = QueryEngine()
                
                # Act: Execute query
                response = engine.query(question)
                
                # Assert: Verify sources is empty
                assert 'sources' in response, "Response must include 'sources' field"
                assert response['sources'] == [], "Sources must be empty when no results found"
    
    @given(
        question=question_strategy(),
        results=query_results_list_strategy()
    )
    @settings(max_examples=100, deadline=None)
    def test_source_metadata_completeness(self, question, results):
        """
        Test that source metadata includes all relevant fields from the original results.
        
        This extends Property 23 to verify metadata completeness.
        Note: Only results with similarity_score >= 0.5 are included in sources.
        """
        # Arrange: Create query engine with mocked dependencies
        with patch('backend.query_engine.get_embedding_engine') as mock_emb:
            with patch('backend.query_engine.get_vector_store') as mock_vs:
                # Mock embedding generation
                mock_emb.return_value.generate_embedding.return_value = [0.1] * 384
                
                # Mock vector store to return our generated results
                mock_vs.return_value.query.return_value = results
                
                engine = QueryEngine()
                
                # Act: Execute query
                response = engine.query(question)
                
                # Filter results to only those with similarity_score >= 0.5 (matching the engine's behavior)
                filtered_results = [r for r in results if r.similarity_score >= 0.5]
                
                # Assert: Verify metadata completeness
                assert len(response['sources']) == len(filtered_results), \
                    f"Number of sources must match filtered results (score >= 0.5)"
                
                for i, source in enumerate(response['sources']):
                    metadata = source['metadata']
                    original_metadata = filtered_results[i].metadata
                    
                    # Check that file_type is included
                    assert 'file_type' in metadata, f"Source {i} metadata must include 'file_type'"
                    assert metadata['file_type'] == original_metadata.get('file_type'), \
                        f"Source {i} file_type must match original"
                    
                    # Check that folder_path is included
                    assert 'folder_path' in metadata, f"Source {i} metadata must include 'folder_path'"
                    assert metadata['folder_path'] == original_metadata.get('folder_path'), \
                        f"Source {i} folder_path must match original"
                    
                    # Check type-specific metadata
                    if original_metadata.get('merchant'):
                        assert 'merchant' in metadata, f"Source {i} metadata must include 'merchant' if present in original"
                        assert metadata['merchant'] == original_metadata['merchant']
                    
                    if original_metadata.get('date'):
                        assert 'date' in metadata, f"Source {i} metadata must include 'date' if present in original"
                        assert metadata['date'] == original_metadata['date']
                    
                    if original_metadata.get('total_amount'):
                        assert 'total_amount' in metadata, f"Source {i} metadata must include 'total_amount' if present in original"
                        assert metadata['total_amount'] == original_metadata['total_amount']
                    
                    if original_metadata.get('page_number'):
                        assert 'page_number' in metadata, f"Source {i} metadata must include 'page_number' if present in original"
                        assert metadata['page_number'] == original_metadata['page_number']


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
